{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import glob\n",
    "import nltk\n",
    "import glob\n",
    "import cv2\n",
    "import spacy\n",
    "import spacy.cli\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import easyocr\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import textseg as ts\n",
    "from spacy import displacy\n",
    "from PyPDF2 import PdfFileReader\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# spacy.cli.download(\"en_core_web_lg\")\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe' #tesseract.exe location in your computer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# SKILLS EXTRACTION\n",
    "# Add skills database from a file\n",
    "def add_skills_data(filePath):\n",
    "    skills = []\n",
    "\n",
    "    for data in open(filePath, 'r', encoding='UTF-8'):\n",
    "        skills.append(data.strip())\n",
    "\n",
    "    return skills\n",
    "\n",
    "# Get the text from a file\n",
    "def extract_text(filePath, remove_line=False):\n",
    "    with fitz.open(filePath) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "\n",
    "        if remove_line:\n",
    "            text = text = re.sub('\\s', \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Extract the skills based on the skill database\n",
    "def extract_skills(input_text, skills_data):\n",
    "    stop_word = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_word]\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    skills = set()\n",
    "\n",
    "    for token in filtered_tokens:\n",
    "        if token in skills_data:\n",
    "            skills.add(token)\n",
    "\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram in skills_data:\n",
    "            skills.add(ngram)\n",
    "\n",
    "    return skills\n",
    "\n",
    "# Extract skills from a single file\n",
    "def extract_single_skills(filePath, skills):\n",
    "    text = extract_text(filePath)\n",
    "\n",
    "    return  extract_skills(text, skills)\n",
    "\n",
    "# Extract skills from a folder full of pdf\n",
    "def extract_batch_skill(filePath, skills):\n",
    "    data = {\"File\": [], 'Skill': []}\n",
    "\n",
    "    for file in glob.glob('{}*.pdf'.format(filePath)):\n",
    "        text = extract_text(file, True)\n",
    "        data['File'].append(file)\n",
    "        data['Skill'].append(extract_skills(text, skills))\n",
    "\n",
    "    return data\n",
    "\n",
    "# DOCUMENT SEGMENTATION\n",
    "# Converting from pdf to image for segmentation\n",
    "def convert_pdf_to_image(filepath,img_path_to_save):\n",
    "    try:\n",
    "        fileName = filepath.split(\"/\")[-1].replace(\".pdf\",\"\")\n",
    "        pages = convert_from_path(filepath, 350)\n",
    "        i = 1\n",
    "        for page in pages:\n",
    "            image_name = img_path_to_save+fileName+\"Page_\" + str(i) + \".png\"\n",
    "            page.save(image_name, \"JPEG\")\n",
    "            i = i+1\n",
    "        return {\"status\":200,\"response\":\"PDF Converted to image sucessfully\",\"fileName\":fileName}\n",
    "    except Exception as e:\n",
    "        return {\"status\":400,\"response\":str(e)}\n",
    "\n",
    "# Extract text from a png\n",
    "def text_from_tesseract(output_img):\n",
    "    text = str(((pytesseract.image_to_string(output_img))))\n",
    "    return text\n",
    "\n",
    "def text_from_easyocr(img, reader):\n",
    "    all_text = \"\"\n",
    "    result = reader.readtext(img)\n",
    "\n",
    "    for (bbox, text, prob) in result:\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    return all_text\n",
    "\n",
    "\n",
    "# Segment and then extract the data from a resume\n",
    "def segment_extract_data(data,  path_to_write, reader, singleFile=True):\n",
    "    documents = [] # file path nya untuk pdf\n",
    "\n",
    "    if singleFile:\n",
    "        documents.append(data)\n",
    "    else:\n",
    "        documents = data\n",
    "\n",
    "    final_name_list=[] # nama file\n",
    "    final_text_opencv=[] # text dengan segmen\n",
    "    # final_text_tessaract=[]\n",
    "    final_text_easyocr=[] # semua text tanpa segmen\n",
    "    for i in documents:\n",
    "        pdf = PdfFileReader(open(i,'rb'))\n",
    "        fname = i.split('/')[-1]\n",
    "\n",
    "        # if pdf.getNumPages() > 3:\n",
    "        #     print('Pages to many : {}!, Skipping...'.format(pdf.getNumPages()))\n",
    "        #     continue\n",
    "\n",
    "        images = convert_from_path(i)\n",
    "        resumes_img=[]\n",
    "        for j in range(len(images)):\n",
    "            # Save pages as images in the pdf\n",
    "            images[j].save(path_to_write+fname.split('.')[0]+'_'+ str(j) +'.jpg', 'JPEG')\n",
    "            resumes_img.append(path_to_write+fname.split('.')[0]+'_'+ str(j) +'.jpg')\n",
    "        name_list = fname.split('.')[0]+'_' +'.jpg'\n",
    "        text_opencv=[]\n",
    "        # text_tessaract=[]\n",
    "        text_easyocr=[]\n",
    "        for i in resumes_img:\n",
    "            frame=cv2.imread(i)\n",
    "            os.remove(i)\n",
    "            img = i.split(\"/\")[2]\n",
    "\n",
    "            output_img,label,dilate, c_dict,df1, split_img=ts.get_text_seg(frame, img)\n",
    "            cv2.imwrite(path_to_write+img.split('.')[0]+\".png\",output_img)\n",
    "            for i in range(len(split_img)):\n",
    "                cv2.imwrite(path_to_write+img.split('.')[0]+str(i)+\".png\", split_img[i])\n",
    "\n",
    "            text_opencv.append(c_dict)\n",
    "            # text_tessaract+=text_from_tesseract(output_img)\n",
    "            # tesseract_str = ''.join(text_tessaract)\n",
    "            text_easyocr+=text_from_easyocr(output_img, reader)\n",
    "            easyocr_str = ''.join(text_easyocr)\n",
    "\n",
    "        final_name_list.append(name_list)\n",
    "        final_text_opencv.append(text_opencv)\n",
    "        # final_text_tessaract.append(tesseract_str)\n",
    "        final_text_easyocr.append(easyocr_str)\n",
    "\n",
    "    return final_text_opencv, final_name_list, final_text_easyocr\n",
    "\n",
    "# EXPERIENCE EXTRACTION\n",
    "# Extract exp from a text\n",
    "def extract_exp(textList, nlp):\n",
    "    exp = []\n",
    "\n",
    "    for i in range(len(textList)):\n",
    "        for j in range(len(textList[i])):\n",
    "            for _, text in textList[i][j].items():\n",
    "                text = re.sub(r'[^\\w\\s]+', \"\", text)\n",
    "                text = re.sub(r'[\\s]{2,}', \" \", text)\n",
    "                text = re.sub(r'https\\w+', \"\", text)\n",
    "                doc = nlp(text)\n",
    "                if doc.cats['experience'] > 0.70:\n",
    "                    exp.append(text)\n",
    "\n",
    "    return exp\n",
    "\n",
    "# Do all the above with just 1 function\n",
    "def extract_data(filePath, skills, nlp, temp_path, reader):\n",
    "    file_data = {'File': \"\", 'Skills':\"\", \"Exp\":\"\"}\n",
    "\n",
    "    textList, fileName, fullText = segment_extract_data(filePath, temp_path, reader)\n",
    "    file_data['File'] = fileName[0]\n",
    "    file_data['Skills'] = extract_skills((fullText[0]), skills_data=skills)\n",
    "    file_data['Exp'] = extract_exp(textList, nlp)\n",
    "\n",
    "    return file_data\n",
    "\n",
    "def batch_extract_data(filePath, skills, nlp, temp_path):\n",
    "    file_data = {'File': [], 'Skills': [], \"Exp\": []}\n",
    "\n",
    "    for file in os.listdir(filePath):\n",
    "        data = extract_data('{}/{}'.format(filePath, file), skills, nlp, temp_path)\n",
    "        file_data['File'].append(data['File'])\n",
    "        file_data['Skills'].append(data['Skills'])\n",
    "        file_data['Exp'].append(data['Exp'])\n",
    "\n",
    "    return file_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU device\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'File': '(1) CV_Debora Musica Karen - Debora Panjaitan_.jpg',\n 'Skills': {'Birth',\n  'Business Partner Support',\n  'Capital',\n  'Email',\n  'General',\n  'HRIS',\n  'Human Capital',\n  'Interview',\n  'Learning',\n  'Mobile',\n  'Onboarding',\n  'Partner Support',\n  'People Development',\n  'Psychology'},\n 'Exp': [' Responsible for onboarding and partnering with the recruitment team in\\nhiring associates based on talent mapping sourcing etc Handle endtoend recruitment for internship Shortlist Interview Offering\\nInduction Onboarding payroll Maintain and updating employee database Responsible for employee documents Termination Extend Internship\\nVacancy Claims ']}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding skills database\n",
    "skills = add_skills_data('list_of_skills.txt')\n",
    "skills[0] = '.NET' # First skills is not UTF-8 so we need to replace it\n",
    "\n",
    "# Segmentation need a temp folder for storing image that will be scanned for extraction the text\n",
    "temp_path = ('./segmentation/')\n",
    "\n",
    "# Load the machine learning model for exp classification\n",
    "nlp = spacy.load('model_best')\n",
    "\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Get the filename, skills, exp\n",
    "data = extract_data('./Lampirkan CV terupdate Anda (File responses)/(1) CV_Debora Musica Karen - Debora Panjaitan.pdf', skills, nlp, temp_path, reader)\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m i \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLampirkan CV terupdate Anda (File responses)\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m----> 4\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[43mextract_text\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLampirkan CV terupdate Anda (File responses)/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(file))\n\u001B[0;32m      5\u001B[0m     text \u001B[38;5;241m=\u001B[39m text\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m     text \u001B[38;5;241m=\u001B[39m text\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[^a-zA-Z0-9]\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m);\n",
      "\u001B[1;31mNameError\u001B[0m: name 'extract_text' is not defined"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_lg\")\n",
    "nlp2.add_pipe(\"language_detector\")\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file in os.listdir('Lampirkan CV terupdate Anda (File responses)'):\n",
    "    text = extract_text('Lampirkan CV terupdate Anda (File responses)/{}'.format(file))\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"[^a-zA-Z0-9]\", \" \");\n",
    "    re.sub('\\W+','', text)\n",
    "    text = text.lower()\n",
    "    doc2 = nlp2(text)\n",
    "    if doc2._.language['language'] == 'en':\n",
    "        try:\n",
    "            print('file is {}'.format(file))\n",
    "            data = extract_data('Lampirkan CV terupdate Anda (File responses)/{}'.format(file), skills, nlp, temp_path)\n",
    "            for j in data['Exp']:\n",
    "                i += 1\n",
    "                f = open('{}.txt'.format(i), \"w+\")\n",
    "                f.write(j)\n",
    "                f.close()\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        print('pass')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}